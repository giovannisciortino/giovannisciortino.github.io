<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2020-04-20T08:09:32+00:00</updated><id>/feed.xml</id><title type="html">SYSADMIN CONTINUOUS IMPROVEMENT</title><subtitle>Useful tips and tools for system administrator</subtitle><entry><title type="html">MODIFICA EFFETTUATA NEL BRANCH TEST</title><link href="/2020/04/05/use_github_actions_to_create_jekyll_post_from_github_issue.html" rel="alternate" type="text/html" title="MODIFICA EFFETTUATA NEL BRANCH TEST" /><published>2020-04-05T00:00:00+00:00</published><updated>2020-04-05T00:00:00+00:00</updated><id>/2020/04/05/use_github_actions_to_create_jekyll_post_from_github_issue</id><content type="html" xml:base="/2020/04/05/use_github_actions_to_create_jekyll_post_from_github_issue.html">&lt;p&gt;This post describe a GitHub Actions workflow that allow to create new post on a Jekyll web site contained in a GitHub repository using the issue editor of GitHub website.&lt;/p&gt;

&lt;p&gt;GitHub Actions [1] makes it easy to automate all your software workflows, it build, test, and deploy your code right from GitHub. 
Jekyll [2] is a simple, blog-aware, static site generator perfect for personal, project, or organization sites.&lt;/p&gt;

&lt;p&gt;The article [3], describing the disadvantages of static site generator software compared to CMS software, includes among them the following “Publishing the site requires tools and code on your computer”. 
The GitHub Actions workflow described in this post allows to mitigate this problem. This workflow allows to use the web interface of GitHub to create Jekyll posts exploiting the “MarkDown editor” and “check spelling” features contained in the issue editor of GitHub web site without require any tool installed.&lt;/p&gt;

&lt;p&gt;The automation described in this post is contained in three files:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A github issue template [4] containing a template of a new jekyll post file&lt;/li&gt;
  &lt;li&gt;A github issue configuration file [5] that allow to create also github issue not respecting the template described in 1.&lt;/li&gt;
  &lt;li&gt;A Github Workflow [6]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The GitHub workflow has been reported also below in order to describe its main components:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-yaml&quot; data-lang=&quot;yaml&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;A workflow to create a jekyll post from github issue&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;issue_comment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;types&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;created&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;build&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;A job to create a jekyll post from github issue&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;github.event.comment.body == 'publish'&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Checkout master branch&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@master&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;

      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Create jekyll post file&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;success() &amp;amp;&amp;amp; github.event.issue.user.login == env.GITHUB_ACCOUNT_JEKYLL_OWNER&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;cat &amp;lt;&amp;lt; 'EOF' &amp;gt; $POST_DIRECTORY/$&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;GITHUB_ACCOUNT_JEKYLL_OWNER&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;giovannisciortino'&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;POST_DIRECTORY&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;_posts'&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Commit files&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;success()&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;git config --local user.name &quot;$GIT_USER_NAME&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;git config --local user.email &quot;$GIT_USER_EMAIL&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;git add --all&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;# commit only if there are changes&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;if [[ `git status --porcelain` ]]; then&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;git commit -m &quot;$DEFAULT_COMMIT_MESSAGE&quot; -a&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;fi&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;GIT_USER_NAME&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Giovanni&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Sciortino'&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;GIT_USER_EMAIL&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;giovannibattistasciortino@gmail.com'&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;DEFAULT_COMMIT_MESSAGE&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;New&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;jekyll&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;github&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;issue'&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Push changes&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;success()&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ad-m/github-push-action@master&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;github_token&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;$&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This GitHub workflow contains the following main elements:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it contains the trigger executing it (line 3-4). It’s triggered when a new comment is added to a github issue&lt;/li&gt;
  &lt;li&gt;it execute the four steps of the job contained in the workflow when someone write the comment “publish” in the GitHub issue (line 9)&lt;/li&gt;
  &lt;li&gt;The first step clone the repository containing the Jekyll git repository in a ubuntu docker container (line 11-12)&lt;/li&gt;
  &lt;li&gt;The second step verify that the author of the command publish is the owner of the GitHub repository and create the Jekyll post reading the content from the title and the first message of the github issue (line 14-22)&lt;/li&gt;
  &lt;li&gt;The third step create a new commit using the file modified in the second step (line 24-37)&lt;/li&gt;
  &lt;li&gt;The forth sep push the new commit to GitHub (line 39-43)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These three files allow to implement the workflow described in this post.
The automation described in this post has been used to create this post itself, [7] is the GitHub issue used to create this post.&lt;/p&gt;

&lt;p&gt;[1] https://github.com/features/actions&lt;/p&gt;

&lt;p&gt;[2] https://jekyllrb.com/&lt;/p&gt;

&lt;p&gt;[3] https://www.strattic.com/jekyll-hugo-wordpress-pros-cons-static-site-generators/&lt;/p&gt;

&lt;p&gt;[4] https://github.com/giovannisciortino/giovannisciortino.github.io/blob/master/.github/ISSUE_TEMPLATE/jekill_post_new_template.md&lt;/p&gt;

&lt;p&gt;[5] https://github.com/giovannisciortino/giovannisciortino.github.io/blob/master/.github/ISSUE_TEMPLATE/config.yml&lt;/p&gt;

&lt;p&gt;[6] https://github.com/giovannisciortino/giovannisciortino.github.io/blob/master/.github/workflows/create_jekyll_post_from_issue.yamlhttps://raw.githubusercontent.com/giovannisciortino/giovannisciortino.github.io/master/.github/workflows/create_jekyll_post_from_issue.yaml&lt;/p&gt;</content><author><name></name></author><category term="github actions" /><category term="jekyll" /><summary type="html">This post describe a GitHub Actions workflow that allow to create new post on a Jekyll web site contained in a GitHub repository using the issue editor of GitHub website. GitHub Actions [1] makes it easy to automate all your software workflows, it build, test, and deploy your code right from GitHub. Jekyll [2] is a simple, blog-aware, static site generator perfect for personal, project, or organization sites. The article [3], describing the disadvantages of static site generator software compared to CMS software, includes among them the following “Publishing the site requires tools and code on your computer”. The GitHub Actions workflow described in this post allows to mitigate this problem. This workflow allows to use the web interface of GitHub to create Jekyll posts exploiting the “MarkDown editor” and “check spelling” features contained in the issue editor of GitHub web site without require any tool installed. The automation described in this post is contained in three files: A github issue template [4] containing a template of a new jekyll post file A github issue configuration file [5] that allow to create also github issue not respecting the template described in 1. A Github Workflow [6] The GitHub workflow has been reported also below in order to describe its main components: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 name: A workflow to create a jekyll post from github issue on: issue_comment: types: [created] jobs: build: name: A job to create a jekyll post from github issue runs-on: ubuntu-latest if: github.event.comment.body == 'publish' steps: - name: Checkout master branch uses: actions/checkout@master - name: Create jekyll post file if: success() &amp;amp;&amp;amp; github.event.issue.user.login == env.GITHUB_ACCOUNT_JEKYLL_OWNER run: | cat &amp;lt;&amp;lt; 'EOF' &amp;gt; $POST_DIRECTORY/$ $ EOF env: GITHUB_ACCOUNT_JEKYLL_OWNER: 'giovannisciortino' POST_DIRECTORY: '_posts' - name: Commit files if: success() run: | git config --local user.name &quot;$GIT_USER_NAME&quot; git config --local user.email &quot;$GIT_USER_EMAIL&quot; git add --all # commit only if there are changes if [[ `git status --porcelain` ]]; then git commit -m &quot;$DEFAULT_COMMIT_MESSAGE&quot; -a fi env: GIT_USER_NAME: 'Giovanni Sciortino' GIT_USER_EMAIL: 'giovannibattistasciortino@gmail.com' DEFAULT_COMMIT_MESSAGE: 'New jekyll post create from github issue' - name: Push changes if: success() uses: ad-m/github-push-action@master with: github_token: $ This GitHub workflow contains the following main elements: it contains the trigger executing it (line 3-4). It’s triggered when a new comment is added to a github issue it execute the four steps of the job contained in the workflow when someone write the comment “publish” in the GitHub issue (line 9) The first step clone the repository containing the Jekyll git repository in a ubuntu docker container (line 11-12) The second step verify that the author of the command publish is the owner of the GitHub repository and create the Jekyll post reading the content from the title and the first message of the github issue (line 14-22) The third step create a new commit using the file modified in the second step (line 24-37) The forth sep push the new commit to GitHub (line 39-43) These three files allow to implement the workflow described in this post. The automation described in this post has been used to create this post itself, [7] is the GitHub issue used to create this post. [1] https://github.com/features/actions [2] https://jekyllrb.com/ [3] https://www.strattic.com/jekyll-hugo-wordpress-pros-cons-static-site-generators/ [4] https://github.com/giovannisciortino/giovannisciortino.github.io/blob/master/.github/ISSUE_TEMPLATE/jekill_post_new_template.md [5] https://github.com/giovannisciortino/giovannisciortino.github.io/blob/master/.github/ISSUE_TEMPLATE/config.yml [6] https://github.com/giovannisciortino/giovannisciortino.github.io/blob/master/.github/workflows/create_jekyll_post_from_issue.yamlhttps://raw.githubusercontent.com/giovannisciortino/giovannisciortino.github.io/master/.github/workflows/create_jekyll_post_from_issue.yaml</summary></entry><entry><title type="html">Distributed SQL Query using SparkSQL, HDFS and Sqoop</title><link href="/2016/03/01/distrubuted_sql_query_using_sparksql_hdfs_sqoop.html" rel="alternate" type="text/html" title="Distributed SQL Query using SparkSQL, HDFS and Sqoop" /><published>2016-03-01T00:00:00+00:00</published><updated>2016-03-01T00:00:00+00:00</updated><id>/2016/03/01/distrubuted_sql_query_using_sparksql_hdfs_sqoop</id><content type="html" xml:base="/2016/03/01/distrubuted_sql_query_using_sparksql_hdfs_sqoop.html">&lt;h2 id=&quot;spark-sql-a-brief-introduction&quot;&gt;Spark SQL: A brief introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/sql/&quot;&gt;Spark SQL&lt;/a&gt; is a component of Spark framework. It allows to manipulate big unstructured data file and extract useful information using SQL.
It introduces a new data abstraction called DataFrames allowing the analysis of structured and semi-structured data.
Spark SQL provides API in Scala,Python and Java in order to manipulate DataFrames.
It also provides the support for SQL language, a command line interface and an ODBC/JDBC server.
The example described in this post shows how to write a simple Spark application in order to execute an SQL query using Spark.&lt;/p&gt;

&lt;h2 id=&quot;import-mysql-data-into-hdfs&quot;&gt;Import MySQL data into HDFS&lt;/h2&gt;

&lt;p&gt;In this paragraph I show how import a MySQL database in hadoop using &lt;a href=&quot;http://sqoop.apache.org/&quot;&gt;sqoop&lt;/a&gt; in the following paragraph I use this data loaded in HDFS in order to execute an SQL query.&lt;/p&gt;

&lt;p&gt;I’m using the “world” database that can be downloaded from this [link] (https://dev.mysql.com/doc/index-other.html). It contains data about cities and countries around the world and the languages spoken in each country.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2016-03-01-distrubuted_sql_query_using_sparksql_hdfs_sqoop_img1.png&quot; alt=&quot;example entity relationship diagram&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I import all tables of world database in hdfs using as output format a text file separated by tab character. The following command imports all the table in the hdfs directory /user/cloudera/world&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;sqoop import-all-tables &lt;span class=&quot;nt&quot;&gt;--connect&lt;/span&gt; jdbc:mysql://localhost/world &lt;span class=&quot;nt&quot;&gt;--username&lt;/span&gt; root &lt;span class=&quot;nt&quot;&gt;-P&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--warehouse-dir&lt;/span&gt;  /user/cloudera/world  &lt;span class=&quot;nt&quot;&gt;--fields-terminated-by&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\t'&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;As you can observe watching the following command Sqoop has created a sub directory for each MySQL table and it has divided table data in different files with the same prefix “part-m-“.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-ls&lt;/span&gt; /user/cloudera/world
Found 3 items
drwxr-xr-x   - cloudera cloudera          0 2016-02-29 21:54 /user/cloudera/world/City
drwxr-xr-x   - cloudera cloudera          0 2016-02-29 21:55 /user/cloudera/world/Country
drwxr-xr-x   - cloudera cloudera          0 2016-02-29 21:55 /user/cloudera/world/CountryLanguage

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-ls&lt;/span&gt; /user/cloudera/world/City
Found 5 items
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera          0 2016-02-29 21:54 /user/cloudera/world/City/_SUCCESS
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera      37088 2016-02-29 21:54 /user/cloudera/world/City/part-m-00000
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera      35361 2016-02-29 21:54 /user/cloudera/world/City/part-m-00001
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera      35884 2016-02-29 21:54 /user/cloudera/world/City/part-m-00002
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera      36148 2016-02-29 21:54 /user/cloudera/world/City/part-m-00003

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-cat&lt;/span&gt;  /user/cloudera/world/City/part-m-00000|head
1       Kabul   AFG     Kabol   1780000
2       Qandahar        AFG     Qandahar        237500
3       Herat   AFG     Herat   186800
4       Mazar-e-Sharif  AFG     Balkh   127800
5       Amsterdam       NLD     Noord-Holland   731200
6       Rotterdam       NLD     Zuid-Holland    593321
7       Haag    NLD     Zuid-Holland    440900
8       Utrecht NLD     Utrecht 234323
9       Eindhoven       NLD     Noord-Brabant   201843
10      Tilburg NLD     Noord-Brabant   193238&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;spark-sql-application&quot;&gt;Spark SQL application&lt;/h2&gt;

&lt;p&gt;This paragraph describes the simple application that I wrote in order to execute the SQL Query using Spark SQL on the HDFS data imported in the last paragraph of this post.&lt;/p&gt;

&lt;p&gt;The SQL query performs a join between all the three table of the database and it allows to extract the top ten country names and their capitals ordered by life expectancy where more than 50% of people speaks English.&lt;/p&gt;

&lt;p&gt;First of all, i created the function “loadCSV” in order to load the data from HDFS in my application. This function accepts three parameters: the HDFS location, the name of the table used in Spark SQL and a lamba function mapping each field of HDFS text files in a specific type (string,integer,float) of a table. It parse each line of the text file in the HDFS location, split them by line and parse each line in order to extract the content of each field; at the end this function register the data on a Spark temporary table.&lt;/p&gt;

&lt;p&gt;Later I execute the query described above and save the result on a &lt;a href=&quot;https://parquet.apache.org/&quot;&gt;parquet&lt;/a&gt; file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark.sql&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Row&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Simple SparqSQL Join&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loadCSV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tableName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;textFile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;schema&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;registerTempTable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tableName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;loadCSV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/user/giovanni/world/City/*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;City&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;countrycode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;district&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loadCSV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/user/giovanni/world/CountryLanguage/*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CountryLanguage&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;countrycode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;language&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;isofficial&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;percentage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loadCSV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/user/giovanni/world/Country/*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Country&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;code&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;continent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;region&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;surfacearea&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'null'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;indepyear&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'null'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;lifeexpectancy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'null'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;gnp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;gnpold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'null'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;localname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;governmentform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;headofstate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;capital&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'null'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;code2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;outputResult&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;SELECT  Country.name as CountryName,
           Country.lifeexpectancy as CountryLifeExpectancy,
           City.name as CapitalName,
           CountryLanguage.language
    FROM
           Country JOIN City on Country.capital = City.id
    JOIN
           CountryLanguage ON CountryLanguage.countrycode = Country.code
    WHERE  CountryLanguage.language =&quot;English&quot;
           AND CountryLanguage.percentage &amp;gt; 50
    ORDER BY CountryLifeExpectancy desc limit 10&quot;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;outputResult&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sparkSQLResult&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;execute-the-application-and-view-the-result&quot;&gt;Execute the application and view the result&lt;/h2&gt;

&lt;p&gt;To run the application in my Hadoop cluster I simply wrote the source code described in the previous paragraph in the file spark_sql.py and I run it using the command spark-submit&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;spark-submit /home/cloudera/workspace/pyspark_examples/spark_sql.py&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next snippet use &lt;a href=&quot;https://github.com/Parquet/parquet-mr/tree/master/parquet-tools&quot;&gt;parquet tools&lt;/a&gt; in order to view in a human readable format the result stored in the parquet file.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;hadoop fs &lt;span class=&quot;nt&quot;&gt;-ls&lt;/span&gt; /user/cloudera/sparkSQLResult
Found 4 items
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera          0 2016-02-29 22:07 /user/cloudera/sparkSQLResult/_SUCCESS
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera        496 2016-02-29 22:07 /user/cloudera/sparkSQLResult/_common_metadata
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera        866 2016-02-29 22:07 /user/cloudera/sparkSQLResult/_metadata
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt;   1 cloudera cloudera       1371 2016-02-29 22:07 /user/cloudera/sparkSQLResult/part-r-00001.parquet

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@quickstart ~]# hadoop parquet.tools.Main &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /user/cloudera/sparkSQLResult
CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Australia
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 79.8
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Canberra
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Canada
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 79.4
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Ottawa
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Gibraltar
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 79.0
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Gibraltar
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Virgin Islands, U.S.
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 78.1
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Charlotte Amalie
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; New Zealand
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 77.8
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Wellington
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; United Kingdom
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 77.7
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; London
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; United States
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 77.1
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Washington
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Bermuda
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 76.9
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Hamilton
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Ireland
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 76.8
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Dublin
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English

CountryName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Belize
CountryLifeExpectancy &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 70.9
CapitalName &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; Belmopan
language &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; English&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><category term="cluster" /><category term="hadoop" /><category term="python" /><category term="spark" /><summary type="html">Spark SQL: A brief introduction</summary></entry><entry><title type="html">Apache Spark – Distributed computation of π in 8 lines of Python code</title><link href="/2016/02/16/apache_spark_distributed_computation_of_pi_in_8_lines_of_python_code.html" rel="alternate" type="text/html" title="Apache Spark – Distributed computation of π in 8 lines of Python code" /><published>2016-02-16T00:00:00+00:00</published><updated>2016-02-16T00:00:00+00:00</updated><id>/2016/02/16/apache_spark_distributed_computation_of_pi_in_8_lines_of_python_code</id><content type="html" xml:base="/2016/02/16/apache_spark_distributed_computation_of_pi_in_8_lines_of_python_code.html">&lt;p&gt;In this post I show how to write a distributed application computing an approximation of pi number though a Spark application using Python.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-introduction-of-spark&quot;&gt;A brief introduction of Spark&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; is an opensource cluster computing framework supporting developers to create distributed applications.
Spark applications provides performance up to 100 times faster compared to Hadoop disk-based map reduce paradigm for certain applications. It allows to load data into a cluster memory and query it repeatedly using different programming language as Python, Scala or Java.&lt;/p&gt;

&lt;h2 id=&quot;a-brief-introduction-of-leibniz-formula-for-pi&quot;&gt;A brief introduction of Leibniz formula for pi&lt;/h2&gt;

&lt;p&gt;This formula also called Leibniz series or Gregory–Leibniz was discovered in 16th century by &lt;a href=&quot;https://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz&quot;&gt;Gottfried Leibniz&lt;/a&gt;) and &lt;a href=&quot;https://en.wikipedia.org/wiki/James_Gregory_(mathematician)&quot;&gt;James Gregory&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2016-02-16-apache_spark_distributed_computation_of_pi_in_8_lines_of_python_code_img1.png&quot; alt=&quot;Gregory-Leibniz-formula&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It allows summing an infinite series of numbers of compute the value of pi divided by 4.&lt;/p&gt;

&lt;p&gt;If you sum a finite series of numbers generated using this formula you can obtain an approximation of pi number divided by 4. Increasing the number of series elements you can obtain a better approximation of pi divided by 4.&lt;/p&gt;

&lt;p&gt;Further details about this formula are present in the Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80&quot;&gt;page&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;pi-approximation-using-apache-spark&quot;&gt;Pi approximation using Apache Spark&lt;/h2&gt;

&lt;p&gt;The following code allow to compute the pi approximation:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pyspark&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;local&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Pi Leibniz approximation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;distIn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;partition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distIn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Pi is &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This source code computes the first 10000 values of Leibniz series and sum it. The result is multiplied for 4 in order to obtain the approximation of pi.
In order to compute the 10000 values of the series, an sequence of integer between 0 and 99999 is generated and stored in distData variable. This sequence of number is splitted in 4 different partitions that could be computed separately by different servers.
In order to compute the i-th element of the series I used the following map function “lambda n: (1 if n % 2 == 0 else -1)/float(2*n+1)”.
The reduce function only sums the elements of the series.&lt;/p&gt;

&lt;p&gt;In order to write the map and the reduce functions, I used lambda function python feature, in this blog i wrote a &lt;a href=&quot;/2015/04/03/python-lambda-functions.html&quot;&gt;post&lt;/a&gt; about python lambda function.
The result obtained from the reduce function is multiplied by 4 and printed to the standard output.&lt;/p&gt;

&lt;h2 id=&quot;development-and-execution-environment&quot;&gt;Development and execution environment&lt;/h2&gt;

&lt;p&gt;In order to develop and execute this application I used Spark 1.3.0&lt;/p&gt;

&lt;p&gt;This version of Spark is present in Cloudera quickstart VM, an virtual machine appliance that contains a test environment for Hadoop/Spark clusters.&lt;/p&gt;

&lt;p&gt;Submitting the code described in this post using this environment is very simple.&lt;/p&gt;

&lt;p&gt;You have to write the source code in a file called for example leibniz_pi.py and execute it running the command “spark-submit leibniz_pi.py”.&lt;/p&gt;

&lt;p&gt;The result of the execution showed in the standard output is:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;Pi is 3.141493&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><category term="hadoop" /><category term="linux" /><category term="python" /><category term="spark" /><summary type="html">In this post I show how to write a distributed application computing an approximation of pi number though a Spark application using Python.</summary></entry><entry><title type="html">Import Mysql data in Elasticsearch server</title><link href="/2016/01/06/import_mysql_data_in_elasticsearch_server.html" rel="alternate" type="text/html" title="Import Mysql data in Elasticsearch server" /><published>2016-01-06T00:00:00+00:00</published><updated>2016-01-06T00:00:00+00:00</updated><id>/2016/01/06/import_mysql_data_in_elasticsearch_server</id><content type="html" xml:base="/2016/01/06/import_mysql_data_in_elasticsearch_server.html">&lt;p&gt;Elasticsearch is a near real-time search server based on Lucene. It allows to create a distributed full-text search engine. It’s an opensource software developed in Java. It offers REST api in order to insert, retrieve and search data.&lt;/p&gt;

&lt;p&gt;In this post I describe how import data from a mysql database in an elasticsearch search engine using the library &lt;a href=&quot;https://github.com/jprante/elasticsearch-jdbc&quot;&gt;https://github.com/jprante/elasticsearch-jdbc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have already installed a Mysql server and an Elasticsearch server, you can find several documentation on internet about installation of these software.&lt;/p&gt;

&lt;p&gt;I use the mysql example database “World” provided by mysql. It can be downloaded from the following url &lt;a href=&quot;http://downloads.mysql.com/docs/world.sql.gz&quot;&gt;http://downloads.mysql.com/docs/world.sql.gz&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This image show the entity relationship model of this database.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2016-01-06-import_mysql_data_in_elasticsearch_server_img1.png&quot; alt=&quot;example entity relationship diagram &quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-1&quot;&gt;Step 1&lt;/h2&gt;
&lt;p&gt;The following text box shows the command used to import the example database in mysql&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@ubuntu01:~/database_example# wget http://downloads.mysql.com/docs/world.sql.gz

root@ubuntu01:~/database_example# unzip world.sql.zip
Archive: world.sql.zip
inflating: world.sql
root@ubuntu01:~/database_example# &lt;span class=&quot;nb&quot;&gt;ls
&lt;/span&gt;world.sql world.sql.zip

root@ubuntu01:~/database_example# mysql &lt;span class=&quot;nt&quot;&gt;-uroot&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&amp;lt;insert here the password&amp;gt;'&lt;/span&gt; world.sql&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;step-2&quot;&gt;Step 2&lt;/h2&gt;

&lt;p&gt;Download the elasticsearch-jdbc library and create the script “mysql-import-world.sh” showed in the following text box and run it in order to import the data from Mysql to Elasticsearch.&lt;/p&gt;

&lt;p&gt;The script contains several parameter:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mysql database connection data (ip, port, database name, username, passowrd)&lt;/li&gt;
  &lt;li&gt;SQL query executed in order to extract the data. In my example I use this query:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-mysql&quot; data-lang=&quot;mysql&quot;&gt;SELECT City.ID as _id,
       City.Name,
       City.District,
       City.Population,
       Country.Name as CountryName,
       Country.continent as CountryContinent
FROM City JOIN Country
ON City.CountryCode = Country.Code;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Elastic search connection data (ip, port)&lt;/li&gt;
  &lt;li&gt;The name of the index created on Elasticsearch&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@ubuntu01:~# wget http://xbib.org/repository/org/xbib/elasticsearch/importer/elasticsearch-jdbc/2.1.0.0/elasticsearch-jdbc-2.1.0.0-dist.zip

root@ubuntu01:~# unzip elasticsearch-jdbc-2.1.0.0-dist.zip
root@ubuntu01:~# &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;elasticsearch-jdbc-2.1.0.0/bin/

root@ubuntu01:~/elasticsearch-jdbc-2.1.0.0/bin# &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ./mysql-import-world.sh
&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dirname&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;BASH_SOURCE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[0]&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/../bin
&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/../lib

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'
{
&quot;type&quot; : &quot;jdbc&quot;,
&quot;jdbc&quot; : {
&quot;url&quot; : &quot;jdbc:mysql://172.17.0.101:3306/world&quot;,
&quot;user&quot; : &quot;root&quot;,
&quot;password&quot; : &quot;password&quot;,
&quot;sql&quot; : &quot;select City.ID as _id,City.Name,City.District,City.Population,Country.Name as CountryName, Country.continent as CountryContinent from City JOIN Country ON City.CountryCode = Country.Code;&quot;,
&quot;treat_binary_as_string&quot; : true,
&quot;elasticsearch&quot; : {
&quot;cluster&quot; : &quot;elasticsearch&quot;,
&quot;host&quot; : &quot;172.17.0.101&quot;,
&quot;port&quot; : 9300
},
&quot;max_bulk_actions&quot; : 20000,
&quot;max_concurrent_bulk_requests&quot; : 10,
&quot;index&quot; : &quot;world&quot;
}
}
'&lt;/span&gt; | java &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-cp&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/*&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-Dlog4j&lt;/span&gt;.configurationFile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bin&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;/log4j2.xml &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
org.xbib.tools.Runner &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
org.xbib.tools.JDBCImporter

root@ubuntu01:~/elasticsearch-jdbc-2.1.0.0/bin# ./mysql-import-world.sh&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;step-3&quot;&gt;Step 3&lt;/h2&gt;

&lt;p&gt;Finally execute a query to the Elasticsearch server in order to verify that the new index world has been created and a second query in order to retrieve some articles from this index.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@ubuntu01:~/elasticsearch-jdbc-2.1.0.0/bin] &lt;span class=&quot;c&quot;&gt;# curl 'http://localhost:9200/_cat/indices?v'&lt;/span&gt;
 health status index pri rep docs.count docs.deleted store.size pri.store.size
 green open world 5 1 4079 0 1.1mb 1.1mb
 green open settings 5 1 0 0 650b 650b

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@ubuntu01:~/elasticsearch-jdbc-2.1.0.0/bin] &lt;span class=&quot;c&quot;&gt;# curl -XGET 'localhost:9200/world/_search?size=3&amp;amp;amp;amp;amp;pretty=true'&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;s2&quot;&gt;&quot;took&quot;&lt;/span&gt; : 2,
 &lt;span class=&quot;s2&quot;&gt;&quot;timed_out&quot;&lt;/span&gt; : &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_shards&quot;&lt;/span&gt; : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;s2&quot;&gt;&quot;total&quot;&lt;/span&gt; : 5,
 &lt;span class=&quot;s2&quot;&gt;&quot;successful&quot;&lt;/span&gt; : 5,
 &lt;span class=&quot;s2&quot;&gt;&quot;failed&quot;&lt;/span&gt; : 0
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;hits&quot;&lt;/span&gt; : &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;s2&quot;&gt;&quot;total&quot;&lt;/span&gt; : 4079,
 &lt;span class=&quot;s2&quot;&gt;&quot;max_score&quot;&lt;/span&gt; : 1.0,
 &lt;span class=&quot;s2&quot;&gt;&quot;hits&quot;&lt;/span&gt; : &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;s2&quot;&gt;&quot;_index&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;world&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_type&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;jdbc&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_id&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;129&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_score&quot;&lt;/span&gt; : 1.0,
 &lt;span class=&quot;s2&quot;&gt;&quot;_source&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Oranjestad&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;District&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Aruba&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Population&quot;&lt;/span&gt;:29034,&lt;span class=&quot;s2&quot;&gt;&quot;CountryName&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Aruba&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;CountryContinent&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;North America&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;, &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;s2&quot;&gt;&quot;_index&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;world&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_type&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;jdbc&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_id&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;60&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_score&quot;&lt;/span&gt; : 1.0,
 &lt;span class=&quot;s2&quot;&gt;&quot;_source&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Namibe&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;District&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Namibe&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Population&quot;&lt;/span&gt;:118200,&lt;span class=&quot;s2&quot;&gt;&quot;CountryName&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Angola&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;CountryContinent&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Africa&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;, &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
 &lt;span class=&quot;s2&quot;&gt;&quot;_index&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;world&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_type&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;jdbc&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_id&quot;&lt;/span&gt; : &lt;span class=&quot;s2&quot;&gt;&quot;73&quot;&lt;/span&gt;,
 &lt;span class=&quot;s2&quot;&gt;&quot;_score&quot;&lt;/span&gt; : 1.0,
 &lt;span class=&quot;s2&quot;&gt;&quot;_source&quot;&lt;/span&gt;:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Lomas de Zamora&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;District&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Buenos Aires&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;Population&quot;&lt;/span&gt;:622013,&lt;span class=&quot;s2&quot;&gt;&quot;CountryName&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Argentina&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;CountryContinent&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;South America&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There are also some software like Kibana and Graphana providing a dashboard really useful to query an elasticsearch server and show the data in a web interface.&lt;/p&gt;</content><author><name></name></author><category term="cluster" /><category term="linux" /><summary type="html">Elasticsearch is a near real-time search server based on Lucene. It allows to create a distributed full-text search engine. It’s an opensource software developed in Java. It offers REST api in order to insert, retrieve and search data.</summary></entry><entry><title type="html">Flume: Import apache logs in hadoop hdfs</title><link href="/2015/07/06/flume_import_apache_logs_in_hadoop_hdfs.html" rel="alternate" type="text/html" title="Flume: Import apache logs in hadoop hdfs" /><published>2015-07-06T00:00:00+00:00</published><updated>2015-07-06T00:00:00+00:00</updated><id>/2015/07/06/flume_import_apache_logs_in_hadoop_hdfs</id><content type="html" xml:base="/2015/07/06/flume_import_apache_logs_in_hadoop_hdfs.html">&lt;p&gt;Flume is a project of the Apache Software Foundation used to import stream of data to a centralized data store. In hadoop environments Flume is used to import data into hadoop clusters from different data sources.&lt;/p&gt;

&lt;p&gt;In this post I show how use Flume to import apache logs (access_log and error_log ) in hadoop hdfs filesystem.&lt;/p&gt;

&lt;p&gt;A Flume agent is composed by a set of sources, channel and sinks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;sources&lt;/strong&gt; are used to collect data/events from different data sources&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;channels&lt;/strong&gt; are the communication media used to temporary store the events collected by the sources&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;sinks&lt;/strong&gt; asynchronously read the events from the channel and send them to a destination&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flume supports different types of sources,channels and sinks. The complete list of sources,channel and sinks already implemented can be obtained reading the documentation ( &lt;a href=&quot;https://flume.apache.org/FlumeUserGuide.html&quot;&gt;https://flume.apache.org/FlumeUserGuide.html&lt;/a&gt; )&lt;/p&gt;

&lt;p&gt;In my example in order to import Apache web server logs I use the following flume component:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Exec source:&lt;/strong&gt; It runs a unix command and expects that process produce data in the standard output&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory channel:&lt;/strong&gt; This channel implements an in-memory queue for the events&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;HDFS Sync:&lt;/strong&gt; It allows to create text file on HDFS
The following image shows the architecture of the flume agent used in this example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2015-07-06-flume_import_apache_logs_in_hadoop_hdfs_img1.jpg&quot; alt=&quot;apache flume architecture&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The flume agent requires a configuration file defining the sources,channels and sinks used by the agent and their properties. The following text box shows the flume configuration file that I used to import apache web server logs in hadoop:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;##### /home/cloudera/example-1.conf&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Name the components on this agent&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# I define two sources:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# a source for access log file&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# a source for for error log file&lt;/span&gt;
agent1.sources &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; tailAccessSource tailErrorSource
&lt;span class=&quot;c&quot;&gt;# I define one sink&lt;/span&gt;
agent1.sinks &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; hdfsSink
&lt;span class=&quot;c&quot;&gt;# I define one channel&lt;/span&gt;
agent1.channels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; memChannel01

&lt;span class=&quot;c&quot;&gt;# Bind the source and sink to the channel&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Both sources will use the memory channel&lt;/span&gt;
agent1.sources.tailAccessSource.channels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; memChannel01
agent1.sources.tailErrorSource.channels &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; memChannel01
agent1.sinks.hdfsSink.channel &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; memChannel01


&lt;span class=&quot;c&quot;&gt;# Define the type and options for each sources&lt;/span&gt;
agent1.sources.tailAccessSource.type &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec
&lt;/span&gt;agent1.sources.tailAccessSource.command &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; /var/log/httpd/access_log

agent1.sources.tailErrorSource.type &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exec
&lt;/span&gt;agent1.sources.tailErrorSource.command &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; /var/log/httpd/error_log

&lt;span class=&quot;c&quot;&gt;# Define the type and options for the channel&lt;/span&gt;
agent1.channels.memChannel01.type &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; memory
agent1.channels.memChannel01.capacity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 100000
agent1.channels.memChannel01.transactionCapacity &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 10000


&lt;span class=&quot;c&quot;&gt;# Define the type and options for the sink&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Note: namenode is the hostname the hadoop namenode server&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#       flume/data-example.1/ is the directory where the apache logs will be stored&lt;/span&gt;
agent1.sinks.hdfsSink.type &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; hdfs
agent1.sinks.hdfsSink.hdfs.path &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; hdfs://namenode/flume/data-example.1/
agent1.sinks.hdfsSink.hdfs.fileType &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; DataStream
agent1.sinks.hdfsSink.hdfs.rollCount &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0
agent1.sinks.hdfsSink.hdfs.rollSize &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0
agent1.sinks.hdfsSink.hdfs.rollInterval &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 60&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The import of apache logs can be started running Flume with its configuration file as argument and waiting that Apache web server start to produce logs:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera@quickstart ~]&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;flume-ng agent &lt;span class=&quot;nt&quot;&gt;--conf&lt;/span&gt; conf &lt;span class=&quot;nt&quot;&gt;--conf-file&lt;/span&gt; /home/cloudera/example-1.conf &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; agent1 &lt;span class=&quot;nt&quot;&gt;-Dflume&lt;/span&gt;.root.logger&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;DEBUG,console&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><category term="apache" /><category term="flume" /><category term="linux" /><summary type="html">Flume is a project of the Apache Software Foundation used to import stream of data to a centralized data store. In hadoop environments Flume is used to import data into hadoop clusters from different data sources.</summary></entry><entry><title type="html">HA two node GPFS cluster with tie-breaker disk</title><link href="/2015/05/23/ha_two_node_gpfs_cluster_with_tie-breaker_disk.html" rel="alternate" type="text/html" title="HA two node GPFS cluster with tie-breaker disk" /><published>2015-05-23T00:00:00+00:00</published><updated>2015-05-23T00:00:00+00:00</updated><id>/2015/05/23/ha_two_node_gpfs_cluster_with_tie-breaker_disk</id><content type="html" xml:base="/2015/05/23/ha_two_node_gpfs_cluster_with_tie-breaker_disk.html">&lt;p&gt;In a previous post I described how configure a GPFS cluster filesystem ( a filesystem that can be mounted by two or more servers simultaneously ).
This article describes the changes required to enable a high-availability configuration for a GPFS cluster filesystem. This configuration allows each node to write and read the filesystem when the other node is down.&lt;/p&gt;

&lt;p&gt;The physical server architecture, showed in the following figure, remains the same:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;two Centos server&lt;/li&gt;
  &lt;li&gt;two shared disks between the servers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2015-05-23-ha_two_node_gpfs_cluster_with_tie-breaker_disk_img1.jpg&quot; alt=&quot;gpfs architecture diagram&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The command mmlscluster output shows that only the first gpfs node has assigned the role of manager and quorum node. In order to enable high-availability both the servers must have these two roles.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlscluster

GPFS cluster information
&lt;span class=&quot;o&quot;&gt;========================&lt;/span&gt;
  GPFS cluster name:         gpfs01
  GPFS cluster &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;:           14526312809412325839
  GPFS UID domain:           gpfs01
  Remote shell &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;:      /usr/bin/ssh
  Remote file copy &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;:  /usr/bin/scp
  Repository &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;:           CCR

 Node  Daemon node name  IP address    Admin node name  Designation
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------&lt;/span&gt;
   1   gpfs01            172.17.0.101  gpfs01           quorum-manager
   2   gpfs02            172.17.0.102  gpfs02&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The filesystem fs_gpfs01 is composed by two network shared disk. In this post I’ll show how configure thee two disks as tie-breaker disks in order to enable the high-availability.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlsnsd &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;

File system Disk name NSD servers
&lt;span class=&quot;nt&quot;&gt;---------------------------------------------------------------------------&lt;/span&gt;
fs_gpfs01 mynsd1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;directly attached&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
fs_gpfs01 mynsd2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;directly attached&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Indeed as many other cluster softwares GPFS requires that the majority of quorum nodes are online to use the filesystem in order to avoid split brain.
In this case the cluster is composed by an even number of cluster nodes so one or more tie-breaker disk must be defined.
More details about gpfs reliability configuration can be found in this document &lt;a href=&quot;http://www-03.ibm.com/systems/resources/configure-gpfs-for-reliability.pdf&quot;&gt;http://www-03.ibm.com/systems/resources/configure-gpfs-for-reliability.pdf&lt;/a&gt; .&lt;/p&gt;

&lt;p&gt;As described before I assign the manager and quorum role to node gpfs02 and I verify it using the command mmlscluster.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# mmchnode &lt;span class=&quot;nt&quot;&gt;--manager&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs02
Thu May 7 22:11:20 CEST 2015: mmchnode: Processing node gpfs02
mmchnode: Propagating the cluster configuration data to all
affected nodes. This is an asynchronous process.

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# mmchnode &lt;span class=&quot;nt&quot;&gt;--quorum&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs02
Thu May 7 22:11:20 CEST 2015: mmchnode: Processing node gpfs02
mmchnode: Propagating the cluster configuration data to all
affected nodes. This is an asynchronous process.
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlscluster

GPFS cluster information
&lt;span class=&quot;o&quot;&gt;========================&lt;/span&gt;
GPFS cluster name: gpfs01
GPFS cluster &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;: 14526312809412325839
GPFS UID domain: gpfs01
Remote shell &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: /usr/bin/ssh
Remote file copy &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: /usr/bin/scp
Repository &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: CCR

Node Daemon node name IP address Admin node name Designation
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------&lt;/span&gt;
1 gpfs01 172.17.0.101 gpfs01 quorum-manager
2 gpfs02 172.17.0.102 gpfs02 quorum-manager&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I configure both NSD as tie-breaker disks and I verify it using the command mmlsconfig&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# mmchconfig &lt;span class=&quot;nv&quot;&gt;tiebreakerDisks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;mynsd1;mynsd2&quot;&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# mmlsconfig
Configuration data &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster gpfs01:
&lt;span class=&quot;nt&quot;&gt;--------------------------------------&lt;/span&gt;
clusterName gpfs01
clusterId 14526312809412325839
autoload no
dmapiFileHandleSize 32
minReleaseLevel 4.1.0.4
ccrEnabled &lt;span class=&quot;nb&quot;&gt;yes
&lt;/span&gt;tiebreakerDisks mynsd1&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;mynsd2
adminMode central

File systems &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;cluster gpfs01:
&lt;span class=&quot;nt&quot;&gt;-------------------------------&lt;/span&gt;
/dev/fs_gpfs01&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now the GPFS HA configuration is completed. I can shutdown one node and verify that the other node can write and read the GPFS filesystem.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# mmmount /fs_gpfs01 &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
Thu May 7 22:22:42 CEST 2015: mmmount: Mounting file systems ...

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# ssh gpfs01 shutdown &lt;span class=&quot;nt&quot;&gt;-h&lt;/span&gt; now
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# &lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /fs_gpfs01/
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 fs_gpfs01]# &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-latr&lt;/span&gt;
dr-xr-xr-x 2 root root 8192 Jan 1 1970 .snapshots
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 fs_gpfs01]# &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-latr&lt;/span&gt;
total 1285
dr-xr-xr-x 2 root root 8192 Jan 1 1970 .snapshots
drwxr-xr-x 2 root root 262144 May 7 21:49 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 root root 1048576 May 7 21:50 test1M
dr-xr-xr-x. 24 root root 4096 May 7 21:55 ..&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Furthermore the log in /var/log/messages provides more details about this event. The log below,grabbed on node gpfs02 when I shutdown the node gpfs01, shows that the node gpfs02 detected the failure of the node gpfs01 and it has been elected cluster manager.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# /var/log/messages&lt;/span&gt;
...
May 7 22:25:29 gpfs02 mmfs: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;E] CCR: failed to connect to node 172.17.0.101:1191 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;sock 42 err 1143&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
May 7 22:25:39 gpfs02 mmfs: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;E] CCR: failed to connect to node 172.17.0.101:1191 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;sock 42 err 1143&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
May 7 22:25:39 gpfs02 mmfs: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;E] Node 172.17.0.101 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;gpfs01&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; is being expelled due to expired lease.
May 7 22:25:39 gpfs02 mmfs: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;N] This node &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;172.17.0.102 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;gpfs02&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt; is now Cluster Manager &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;gpfs01.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><category term="cluster" /><category term="linux" /><summary type="html">In a previous post I described how configure a GPFS cluster filesystem ( a filesystem that can be mounted by two or more servers simultaneously ). This article describes the changes required to enable a high-availability configuration for a GPFS cluster filesystem. This configuration allows each node to write and read the filesystem when the other node is down.</summary></entry><entry><title type="html">HAProxy basic configuration on Ubuntu 14.04</title><link href="/2015/05/09/haproxy_basic_configuration_on_ubuntu_14.04.html" rel="alternate" type="text/html" title="HAProxy basic configuration on Ubuntu 14.04" /><published>2015-05-09T00:00:00+00:00</published><updated>2015-05-09T00:00:00+00:00</updated><id>/2015/05/09/haproxy_basic_configuration_on_ubuntu_14.04</id><content type="html" xml:base="/2015/05/09/haproxy_basic_configuration_on_ubuntu_14.04.html">&lt;p&gt;HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites. Over the years it has become the de-facto standard opensource software load balancer, is now shipped with most mainstream Linux distributions, and is often deployed by default in cloud platforms.&lt;/p&gt;

&lt;p&gt;This post describes the steps used to install and deploy a basic configuration of HAProxy on Ubuntu 14.04.&lt;/p&gt;

&lt;p&gt;The architecture is composed by three ubuntu virtual machine:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Hostname:&lt;/strong&gt; node1    &lt;strong&gt;IP Address:&lt;/strong&gt; 172.17.0.101&lt;br /&gt;
&lt;strong&gt;Description:&lt;/strong&gt; Server used to install and to run haproxy&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hostname:&lt;/strong&gt; node2    &lt;strong&gt;IP Address:&lt;/strong&gt; 172.17.0.102 &lt;br /&gt;
&lt;strong&gt;Description:&lt;/strong&gt; Server running already apache on port 80&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hostname:&lt;/strong&gt; node3    &lt;strong&gt;IP Address:&lt;/strong&gt; 172.17.0.103&lt;br /&gt;
&lt;strong&gt;Description:&lt;/strong&gt; Server running already apache on port 80&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2015-05-09-haproxy_basic_configuration_on_ubuntu_14.04_img1.jpg&quot; alt=&quot;haproxy architecture diagram&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The haproxy software on server node1 will be used to balance the apache service installed on node1 and node2.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Install haproxy software&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~# apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;haproxy
Reading package lists... Done
Building dependency tree
Reading state information... Done
Suggested packages:
vim-haproxy
The following NEW packages will be installed:
haproxy
0 upgraded, 1 newly installed, 0 to remove and 74 not upgraded.
Need to get 453 kB of archives.
After this operation, 822 kB of additional disk space will be used.
Get:1 http://it.archive.ubuntu.com/ubuntu/ trusty/main haproxy amd64 1.4.24-2 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;453 kB]
Fetched 453 kB &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;0s &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;568 kB/s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Selecting previously unselected package haproxy.
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Reading database ... 68156 files and directories currently installed.&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Preparing to unpack .../haproxy_1.4.24-2_amd64.deb ...
Unpacking haproxy &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1.4.24-2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
Processing triggers &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;ureadahead &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;0.100.0-16&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
ureadahead will be reprofiled on next reboot
Processing triggers &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;man-db &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;2.6.7.1-1ubuntu1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
Setting up haproxy &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1.4.24-2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
Processing triggers &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;ureadahead &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;0.100.0-16&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Enable haproxy init.d script&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For default haproxy has been disabled. Enable HAProxy editing /etc/default/haproxy and setting ENABLED variable to 1&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~#  &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;s/ENABLED=0/ENABLED=1/g&quot;&lt;/span&gt; /etc/default/haproxy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Add the first frontend and backend in haproxy configuration&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Edit the haproxy configuration file ( /etc/haproxy/haproxy.cfg ) and add the first frontend and backend sections.&lt;/p&gt;

&lt;p&gt;These two sections showed below allow to balance each request coming to 172.17.0.101 port 80 to the ports 80 of the two web servers node2 and node3 (ip addresses 172.17.0.101 172.17.0.102).&lt;/p&gt;

&lt;p&gt;The option “check” in the last two lines is used to verify using a tcp connection that each web server is available. If one of backend server is offline or doesn’t respond to tcp connection on port 80, haproxy redirects all the requests on the other webserver.&lt;/p&gt;

&lt;p&gt;The balancing method used by default in haproxy is round robin.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~# &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/haproxy/haproxy.cfg
global
log /dev/log local0
log /dev/log local1 notice
&lt;span class=&quot;nb&quot;&gt;chroot&lt;/span&gt; /var/lib/haproxy
user haproxy
group haproxy
daemon

defaults
log global
mode http
option httplog
option dontlognull
contimeout 5000
clitimeout 50000
srvtimeout 50000
errorfile 400 /etc/haproxy/errors/400.http
errorfile 403 /etc/haproxy/errors/403.http
errorfile 408 /etc/haproxy/errors/408.http
errorfile 500 /etc/haproxy/errors/500.http
errorfile 502 /etc/haproxy/errors/502.http
errorfile 503 /etc/haproxy/errors/503.http
errorfile 504 /etc/haproxy/errors/504.http

frontend http-frontend
&lt;span class=&quot;nb&quot;&gt;bind &lt;/span&gt;172.17.0.101:80
&lt;span class=&quot;c&quot;&gt;# Add &quot;X-Forwarded-For&quot; header with the original client's IP address&lt;/span&gt;
reqadd X-Forwarded-Proto:&lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;http
default_backend http-backend

backend http-backend
server node2 172.17.0.102:80 check
server node3 172.17.0.103:80 check&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Start haproxy and enable starting on boot&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Start haproxy service&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~# /etc/init.d/haproxy start
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Starting haproxy haproxy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Enable haproxy start on boot&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~# update-rc.d haproxy defaults
Adding system startup &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; /etc/init.d/haproxy ...
/etc/rc0.d/K20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy
/etc/rc1.d/K20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy
/etc/rc6.d/K20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy
/etc/rc2.d/S20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy
/etc/rc3.d/S20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy
/etc/rc4.d/S20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy
/etc/rc5.d/S20haproxy -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; ../init.d/haproxy&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Log haproxy location&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The default location of haproxy.log is /var/log/haproxy.log&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~# &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /var/log/haproxy.log
May 4 21:04:33 node1 haproxy[2270]: Proxy http-frontend started.
May 4 21:04:33 node1 haproxy[2270]: Proxy http-frontend started.
May 4 21:04:33 node1 haproxy[2270]: Proxy http-backend started.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Test haproxy configuration&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can use curl or access to the url &lt;a href=&quot;http://172.17.0.101/&quot;&gt;http://172.17.0.101/&lt;/a&gt; using a browser&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;root@node1:~# curl http://172.17.0.101/&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Enable HAproxy stats&lt;/strong&gt;
HAproxy provides a simple web interface useful to show the status and statistics about the frontend/backend configured in haproxy and their connections.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To enable it add the following content in /etc/haproxy/haproxy.cfg and reload the service :&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;listen stats 172.17.0.101:1936

mode http
log global

maxconn 10

clitimeout 100s
srvtimeout 100s
contimeout 100s
&lt;span class=&quot;nb&quot;&gt;timeout &lt;/span&gt;queue 100s

stats &lt;span class=&quot;nb&quot;&gt;enable
&lt;/span&gt;stats hide-version
stats refresh 30s
stats show-node
stats auth admin:password &lt;span class=&quot;c&quot;&gt;# Change username and password for statistics webpage authentication&lt;/span&gt;
stats uri /haproxy?stats
Reload haproxy configuration

root@node1:~# /etc/init.d/haproxy reload&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;View haproxy statistics using the url &lt;a href=&quot;http://172.17.0.101:1936/haproxy?stats&quot;&gt;http://172.17.0.101:1936/haproxy?stats&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Configure other frontend/backend with different balancing algorithms&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An instance of haproxy can be configured to balance multiple services configuring multiple frontend and backend sections in its configuration file.&lt;/p&gt;

&lt;p&gt;HAproxy allows also to define different balancing algorithms, it use round robin as default but it support other algorithms that can be selected using the option balance in backend section:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;source:&lt;/strong&gt; This method selects which server to use based on a hash of the source IP i.e. your user’s IP address. This is one method to ensure that a user will connect to the same server&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;leastconn:&lt;/strong&gt; This method selects the server with the least number of connections. It’s recommended for longer sessions. Servers in the same backend are also rotated in a round-robin fashion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following configuration can be added to haproxy configuration file in order to add two other balanced service listening respectively on port 81 and 82 using leastconn and source balancing method:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;frontend http-frontend1

&lt;span class=&quot;nb&quot;&gt;bind &lt;/span&gt;172.17.0.101:81
reqadd X-Forwarded-Proto:&lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;http
default_backend http-backend1

backend http-backend1
balance leastconn
server node2 172.17.0.102:80 check
server node3 172.17.0.103:80 check

frontend http-frontend2
&lt;span class=&quot;nb&quot;&gt;bind &lt;/span&gt;172.17.0.101:82
reqadd X-Forwarded-Proto:&lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;http
default_backend http-backend2

backend http-backend2
balance &lt;span class=&quot;nb&quot;&gt;source
&lt;/span&gt;server node2 172.17.0.102:80 check
server node3 172.17.0.103:80 check&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name></name></author><category term="haproxy" /><category term="linux" /><summary type="html">HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites. Over the years it has become the de-facto standard opensource software load balancer, is now shipped with most mainstream Linux distributions, and is often deployed by default in cloud platforms.</summary></entry><entry><title type="html">Install and configure GPFS 4.1 filesystem on Linux Centos 6.6</title><link href="/2015/05/09/install_and_configure_gpfs_4.1_filesystem_on_linux_centos_6.6.html" rel="alternate" type="text/html" title="Install and configure GPFS 4.1 filesystem on Linux Centos 6.6" /><published>2015-05-09T00:00:00+00:00</published><updated>2015-05-09T00:00:00+00:00</updated><id>/2015/05/09/install_and_configure_gpfs_4.1_filesystem_on_linux_centos_6.6</id><content type="html" xml:base="/2015/05/09/install_and_configure_gpfs_4.1_filesystem_on_linux_centos_6.6.html">&lt;p&gt;The General Parallel File System (GPFS) is a high-performance clustered file system developed by IBM. It can be deployed in shared-disk infrastructure or in shared-nothing architecture. It’s used by many large company and in serveral supercomputers on the Top 500 List.&lt;/p&gt;

&lt;p&gt;GPFS allows to configure a high available filesystem allowing concurrent access from a cluster of nodes.
Cluster nodes can be server using AIX, Linux or Windows operatng system.&lt;/p&gt;

&lt;p&gt;GPFS provides high performance allowing striping blocks of data over multiple disk reading and writing this blocks in parallel. It offers also block replication over different disks in order to guarantee the availability of the filesystem also during disk failures.&lt;/p&gt;

&lt;p&gt;The following list contains some of the most interesting features of GPFS:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Provide a POSIX compliant interface&lt;/li&gt;
  &lt;li&gt;Allow filesystem mounting to client accessing data though LAN connection&lt;/li&gt;
  &lt;li&gt;Many filesystem maintenance tasks can be performed while the filesystem is mounted&lt;/li&gt;
  &lt;li&gt;Support quota&lt;/li&gt;
  &lt;li&gt;Distributes metadata over different disks&lt;/li&gt;
  &lt;li&gt;Have really high scalability limits&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post describes the step required to implement a basic configuration of GPFS filesystem on a cluster composed by two Centos 6.6 servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2015-05-09-install_and_configure_gpfs_4.1_filesystem_on_linux_centos_6.6_img1.jpg&quot; alt=&quot;gpfs architecture diagram&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The server architectures showed in the images is composed by two server gpfs01 and gpfs02 sharing two disk device (sdb and sdc) each one of 10 GB size. Each server has an ethernet connection on subnet 172.17.0.0/16 allowing the communication between gpfs cluster nodes. The gpfs version used for installation is 4.1.0-5.&lt;/p&gt;

&lt;p&gt;The following sequence of tasks show how install and configure GPFS on this couple of servers.&lt;/p&gt;

&lt;h2 id=&quot;1-install-gpfs-rpm&quot;&gt;1. Install GPFS rpm&lt;/h2&gt;

&lt;p&gt;Install gpfs rpm on both nodes&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 gpfs_repo]# find &lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;
/root/gpfs_repo
/root/gpfs_repo/gpfs.ext-4.1.0-5.x86_64.rpm
/root/gpfs_repo/gpfs.docs-4.1.0-5.noarch.rpm
/root/gpfs_repo/kmod-gpfs-4.1.0-5.15.sdl6.x86_64.rpm
/root/gpfs_repo/gpfs.base-4.1.0-5.0.1.x86_64.rpm
/root/gpfs_repo/gpfs.gskit-8.0.50-32.x86_64.rpm
/root/gpfs_repo/gpfs.msg.en_US-4.1.0-5.noarch.rpm
/root/gpfs_repo/gpfs.gpl-4.1.0-5.noarch.rpm

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 gpfs_repo]# yum localinstall &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.rpm

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 gpfs_repo]# find &lt;span class=&quot;nv&quot;&gt;$PWD&lt;/span&gt;
/root/gpfs_repo
/root/gpfs_repo/gpfs.ext-4.1.0-5.x86_64.rpm
/root/gpfs_repo/gpfs.docs-4.1.0-5.noarch.rpm
/root/gpfs_repo/kmod-gpfs-4.1.0-5.15.sdl6.x86_64.rpm
/root/gpfs_repo/gpfs.base-4.1.0-5.0.1.x86_64.rpm
/root/gpfs_repo/gpfs.gskit-8.0.50-32.x86_64.rpm
/root/gpfs_repo/gpfs.msg.en_US-4.1.0-5.noarch.rpm
/root/gpfs_repo/gpfs.gpl-4.1.0-5.noarch.rpm

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 gpfs_repo]# yum localinstall &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.rpm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;2-configure-etchosts&quot;&gt;2. Configure /etc/hosts&lt;/h2&gt;

&lt;p&gt;Configure hosts file on both nodes in order to allow name resolution.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 gpfs_repo]# &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/hosts
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
172.17.0.101 gpfs01 gpfs01.example.com
172.17.0.102 gpfs02 gpfs02.example.com

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 gpfs_repo]# &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/hosts
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
172.17.0.101 gpfs01 gpfs01.example.com
172.17.0.102 gpfs02 gpfs02.example.com&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;3-exchange-root-ssh-key-between-nodes&quot;&gt;3. Exchange root ssh key between nodes&lt;/h2&gt;

&lt;p&gt;GPFS requires that each gpfs cluster nodes can execute ssh commands on all other nodes using root user in order to allow remote administration of other nodes. In order to allow it you have to exchange ssh root keys between each cluster node.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# ssh-copy-id root@gpfs01
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# ssh-copy-id root@gpfs02
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# ssh-copy-id root@gpfs01
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# ssh-copy-id root@gpfs02&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;4-test-ssh-password-less-connection&quot;&gt;4. Test ssh password-less connection&lt;/h2&gt;

&lt;p&gt;Verify the previous step executing a ssh connection between each couple of nodes.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# ssh gpfs01 &lt;span class=&quot;nb&quot;&gt;date
&lt;/span&gt;Sat Apr 18 10:52:08 CEST 2015

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# ssh gpfs02 &lt;span class=&quot;nb&quot;&gt;date
&lt;/span&gt;Sat Apr 18 10:52:09 CEST 2015

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# ssh gpfs01 &lt;span class=&quot;nb&quot;&gt;date
&lt;/span&gt;Mon Apr 13 21:44:52 CEST 2015

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# ssh gpfs02 &lt;span class=&quot;nb&quot;&gt;date
&lt;/span&gt;Mon Apr 13 21:44:53 CEST 2015&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;5-compile-gpfs-portability-layer-rpm&quot;&gt;5. Compile GPFS portability layer rpm&lt;/h2&gt;

&lt;p&gt;The GPFS portability layer is a loadable kernel module that allows the GPFS daemon to interact with the operating system.&lt;/p&gt;

&lt;p&gt;IBM provides the source code of this module. It must be compiled for the kernel version used by your servers. This step can be executed on a single node then the rpm containing the kernel module can be distributed and installed over all the other gpfs nodes. In this example this module will be compiled on server gpfs01.&lt;/p&gt;

&lt;p&gt;In order to avoid the error “Cannot determine the distribution type. /etc/redhat-release is present, but the release name is not recognized. Specify the distribution type explicitly.” during module compiling replace content of /etc/redhat-release with the string “Red Hat Enterprise Linux Server release 6.6 (Santiago)”.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# &lt;span class=&quot;nb&quot;&gt;mv&lt;/span&gt; /etc/redhat-release /etc/redhat-release.original
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Red Hat Enterprise Linux Server release 6.6 (Santiago)&quot;&lt;/span&gt; &amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; /etc/redhat-release&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In order to avoid the error “Cannot find a valid kernel include directory” during module compiling install the rpm required for compile module for your kernel version (kernel source, rpmbuild, …).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kernel-headers-2.6.32-504.12.2.el6

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kernel-devel-2.6.32-504.12.2.el6
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;imake gcc-c++ rpmbuild&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;6-install-gpfs-portability-layer-rpm&quot;&gt;6. Install GPFS portability layer rpm&lt;/h2&gt;

&lt;p&gt;Distribute GPFS portability layer rpm on each node and install it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# scp /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-2.6.32-504.12.2.el6.x86_64-4.1.0-5.x86_64.rpm gpfs02:/tmp/gpfs.gplbin-2.6.32-504.12.2.el6.x86_64-4.1.0-5.x86_64.rpm

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 src]# yum localinstall /root/rpmbuild/RPMS/x86_64/gpfs.gplbin-2.6.32-504.12.2.el6.x86_64-4.1.0-5.x86_64.rpm

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs02 ~]# yum localinstall /tmp/gpfs.gplbin-2.6.32-504.12.2.el6.x86_64-4.1.0-5.x86_64.rpm&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;7-create-gpfs-cluster&quot;&gt;7. Create GPFS cluster&lt;/h2&gt;

&lt;p&gt;In this step the cluster is created adding the node gpfs01 with the role of cluster manager and quorum manager. In the next steps the gpfs02 node will be added to the cluster.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmcrcluster &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs01:manager-quorum &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; gpfs01 &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; /usr/bin/ssh &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; /usr/bin/scp

mmcrcluster: Performing preliminary node verification ...
mmcrcluster: Processing quorum and other critical nodes ...
mmcrcluster: Finalizing the cluster data structures ...
mmcrcluster: Command successfully completed
mmcrcluster: Warning: Not all nodes have proper GPFS license designations.
Use the mmchlicense &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;to designate licenses as needed&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlscluster

&lt;span class=&quot;o&quot;&gt;===============================================================================&lt;/span&gt;
| Warning: |
| This cluster contains nodes that &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;not have a proper GPFS license |
| designation. This violates the terms of the GPFS licensing agreement. |
| Use the mmchlicense &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;and assign the appropriate GPFS licenses |
| to each of the nodes &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the cluster. For more information about GPFS |
| license designation, see the Concepts, Planning, and Installation Guide. |
&lt;span class=&quot;o&quot;&gt;===============================================================================&lt;/span&gt;
GPFS cluster information
&lt;span class=&quot;o&quot;&gt;========================&lt;/span&gt;
GPFS cluster name: gpfs01
GPFS cluster &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;: 14526312809412325839
GPFS UID domain: gpfs01
Remote shell &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: /usr/bin/ssh
Remote file copy &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: /usr/bin/scp
Repository &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: CCR

Node Daemon node name IP address Admin node name Designation
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------&lt;/span&gt;
1 gpfs01 172.17.0.101 gpfs01 quorum-manager&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You need to accept the GPFS server license for node gpfs01.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmchlicense server &lt;span class=&quot;nt&quot;&gt;--accept&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs01

The following nodes will be designated as possessing GPFS server licenses:
gpfs01
mmchlicense: Command successfully completed&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;8-start-gpfs-cluster-on-node-gpfs01&quot;&gt;8. Start gpfs cluster on node gpfs01&lt;/h2&gt;

&lt;p&gt;Start gpfs cluster on node gpfs01&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmstartup &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs01
Fri Apr 24 18:43:35 CEST 2015: mmstartup: Starting GPFS ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Verify the node status&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmgetstate &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;

Node number Node name GPFS state
&lt;span class=&quot;nt&quot;&gt;------------------------------------------&lt;/span&gt;
1 gpfs01 active&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;9-add-the-second-node-to-gpfs&quot;&gt;9. Add the second node to GPFS&lt;/h2&gt;

&lt;p&gt;In this step the second server gpfs02 will be added to the gpfs cluster.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmaddnode &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs02
Fri Apr 24 18:44:54 CEST 2015: mmaddnode: Processing node gpfs02
mmaddnode: Command successfully completed
mmaddnode: Warning: Not all nodes have proper GPFS license designations.
Use the mmchlicense &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;to designate licenses as needed.
mmaddnode: Propagating the cluster configuration data to all
affected nodes. This is an asynchronous process.

Now the &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;mmlscluster shows that the cluster is composed by two servers.

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlscluster

GPFS cluster information

&lt;span class=&quot;o&quot;&gt;========================&lt;/span&gt;
GPFS cluster name: gpfs01
GPFS cluster &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;: 14526312809412325839
GPFS UID domain: gpfs01
Remote shell &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: /usr/bin/ssh
Remote file copy &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: /usr/bin/scp
Repository &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: CCR

Node Daemon node name IP address Admin node name Designation
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------&lt;/span&gt;
1 gpfs01 172.17.0.101 gpfs01 quorum-manager
2 gpfs02 172.17.0.102 gpfs02&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;10-start-gpfs-on-gpfs02-node&quot;&gt;10. Start GPFS on gpfs02 node&lt;/h2&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmstartup &lt;span class=&quot;nt&quot;&gt;-N&lt;/span&gt; gpfs02
Fri Apr 24 18:47:32 CEST 2015: mmstartup: Starting GPFS ...

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmgetstate &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;

Node number Node name GPFS state
&lt;span class=&quot;nt&quot;&gt;------------------------------------------&lt;/span&gt;
1 gpfs01 active
2 gpfs02 active&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;11-create-nsd-configuration&quot;&gt;11. Create NSD configuration&lt;/h2&gt;

&lt;p&gt;Now you have to create a file containing the configuration of the disk that will be used by gpfs. The disk used by GPFS are called Network Shared Disk (NSD) using GPFS terminology.&lt;/p&gt;

&lt;p&gt;The file diskdef.txt showed below contain the NSD configuration used by GPFS.&lt;/p&gt;

&lt;p&gt;Two NSD disk has been defined, their name are mynsd1 and mynsd2 and the device files of these disks are respectively /dev/sdb and /dev/sdc. Both disks will be used to store data and metadata.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# &lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;diskdef.txt

%nsd:
&lt;span class=&quot;nv&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/sdb
&lt;span class=&quot;nv&quot;&gt;nsd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mynsd1
&lt;span class=&quot;nv&quot;&gt;usage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dataAndMetadata

%nsd:
&lt;span class=&quot;nv&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/sdc
&lt;span class=&quot;nv&quot;&gt;nsd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mynsd2
&lt;span class=&quot;nv&quot;&gt;usage&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dataAndMetadata&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Configure the NSD using this configuration file&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmcrnsd &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; diskdef.txt
mmcrnsd: Processing disk sdb
mmcrnsd: Processing disk sdc
mmcrnsd: Propagating the cluster configuration data to all
affected nodes. This is an asynchronous process.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Show the NSD configuration&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlsnsd

File system Disk name NSD servers
&lt;span class=&quot;nt&quot;&gt;---------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;free disk&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; mynsd1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;directly attached&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;free disk&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; mynsd2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;directly attached&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;13-create-gpfs-filesystem&quot;&gt;13. Create GPFS filesystem&lt;/h2&gt;

&lt;p&gt;The following command create a gpfs filesystem called fs_gpfs01 using the NSD defined in diskdef.txt file that will be mounted on /fs_gpfs01 mount point&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmcrfs fs_gpfs01 &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; diskdef.txt &lt;span class=&quot;nt&quot;&gt;-A&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-T&lt;/span&gt; /fs_gpfs01

The following disks of fs_gpfs01 will be formatted on node gpfs01.example.com:
mynsd1: size 10240 MB
mynsd2: size 10240 MB
Formatting file system ...
Disks up to size 103 GB can be added to storage pool system.
Creating Inode File
86 % &lt;span class=&quot;nb&quot;&gt;complete &lt;/span&gt;on Fri Apr 24 19:30:27 2015
100 % &lt;span class=&quot;nb&quot;&gt;complete &lt;/span&gt;on Fri Apr 24 19:30:27 2015
Creating Allocation Maps
Creating Log Files
Clearing Inode Allocation Map
Clearing Block Allocation Map
Formatting Allocation Map &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;storage pool system
Completed creation of file system /dev/fs_gpfs01.
mmcrfs: Propagating the cluster configuration data to all
affected nodes. This is an asynchronous process.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;14-mountunmount-gpfs&quot;&gt;14. Mount/Unmount gpfs&lt;/h2&gt;

&lt;p&gt;This step shows some useful command to mount and unmount the gpfs filesystem&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mount on all nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmmount /fs_gpfs01 &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
Thu May 7 21:30:33 CEST 2015: mmmount: Mounting file systems ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Unmount on all nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmumount /fs_gpfs01 &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Mount on local node&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmmount /fs_gpfs01&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Unmount on local node&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmumount /fs_gpfs01&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;15-verify-mount-gpfs-filesystem&quot;&gt;15. Verify mount gpfs filesystem&lt;/h2&gt;

&lt;p&gt;You can verify that gpfs filesystem is mounted using the command mmlsmount or using df&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# /usr/lpp/mmfs/bin/mmlsmount all
File system fs_gpfs01 is mounted on 2 nodes.

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# &lt;span class=&quot;nb&quot;&gt;df
&lt;/span&gt;Filesystem 1K-blocks Used Available Use% Mounted on
/dev/mapper/vg_gpfs01-lv_root 17938864 1278848 15742104 8% /
tmpfs 1958512 0 1958512 0% /dev/shm
/dev/sda1 487652 69865 392187 16% /boot
/dev/fs_gpfs01 20971520 533248 20438272 3% /fs_gpfs01&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;16-log-location&quot;&gt;16. Log location&lt;/h2&gt;

&lt;p&gt;GPFS filesystem are located in the directory gpfs /var/adm/ras&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@gpfs01 ~]# &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-latr&lt;/span&gt; /var/adm/ras
total 20
drwxr-xr-x. 3 root root 4096 Apr 13 16:46 ..
lrwxrwxrwx 1 root root 35 Apr 24 20:33 mmfs.log.previous -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; mmfs.log.2015.04.24.20.33.44.gpfs01
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 root root 3835 May 7 21:11 mmfs.log.2015.04.24.20.33.44.gpfs01
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 root root 195 May 7 21:11 mmsdrserv.log
lrwxrwxrwx 1 root root 35 May 7 21:24 mmfs.log.latest -&amp;amp;gt&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; mmfs.log.2015.05.07.21.24.18.gpfs01
drwxr-xr-x. 2 root root 4096 May 7 21:24 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-rw-r--r--&lt;/span&gt; 1 root root 2717 May 7 21:24 mmfs.log.2015.05.07.21.24.18.gpfs01&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;GPFS records also system and disk failure using syslog, gpfs error log can be retrieved using the command:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;mmfs:&quot;&lt;/span&gt; /var/log/messages&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;17-add-usrlppmmfsbin-to-path-environment-variable&quot;&gt;17. Add /usr/lpp/mmfs/bin/ to PATH environment variable&lt;/h2&gt;

&lt;p&gt;In order to avoid to use the full path for gpfs command the directory /usr/lpp/mmfs/bin/ can be added to the environment PATH variable of root&lt;/p&gt;

&lt;p&gt;Add the line “PATH=$PATH:/usr/lpp/mmfs/bin/” in /root/.bash_profile before the line “export PATH”&lt;/p&gt;</content><author><name></name></author><category term="cluster" /><category term="linux" /><summary type="html">The General Parallel File System (GPFS) is a high-performance clustered file system developed by IBM. It can be deployed in shared-disk infrastructure or in shared-nothing architecture. It’s used by many large company and in serveral supercomputers on the Top 500 List.</summary></entry><entry><title type="html">Install Cloudera prerequisites with Ansible</title><link href="/2015/05/09/install_cloudera_prerequisites_with_ansible.html" rel="alternate" type="text/html" title="Install Cloudera prerequisites with Ansible" /><published>2015-05-09T00:00:00+00:00</published><updated>2015-05-09T00:00:00+00:00</updated><id>/2015/05/09/install_cloudera_prerequisites_with_ansible</id><content type="html" xml:base="/2015/05/09/install_cloudera_prerequisites_with_ansible.html">&lt;p&gt;Ansible is an opensource software for configuring and managing a server infrastructures. It allows multi-node software deployment, ad hoc task execution and configuration management.
In this post I show how use ansible to deploy some Cloudera Hadoop 5 prerequisites to a large set of server.&lt;/p&gt;

&lt;p&gt;Ansible is a lightweight alternative to other opensource configuration management tools like puppet. It doesn’t need any agent installed on each managed node like puppet. It require only a ssh connection from ansible server to managed servers and the python package installed on them.&lt;/p&gt;

&lt;p&gt;In this tutorial two type of ansible configuration files will be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;hosts file:&lt;/strong&gt; it allows to define the list of hosts managed by ansible&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;playbook:&lt;/strong&gt; it contains a list of configurations that can be deployed to a server or a group of server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cloudera Hadoop requires for each nodes the following prerequisites :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The file /etc/hosts of each nodes containing consistent information about hostnames and IP addresses across all hosts&lt;/li&gt;
  &lt;li&gt;SELinux disabled&lt;/li&gt;
  &lt;li&gt;IPv6 disabled&lt;/li&gt;
  &lt;li&gt;vm.swappiness kernel option must be set to a value less or equal to 10&lt;/li&gt;
  &lt;li&gt;A password-less ssh connection to root user with a unique private keys must be configured&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This configuration must be deployed to each cloudera server. The manual execution of these tasks over a large hadoop infrastructure could be a time-consuming activity.
Ansible can automatize all this operation.&lt;/p&gt;

&lt;p&gt;The example showed in this post uses an hostgroup of 4 cloudera server but it can be easily scaled to hundreds of servers.&lt;/p&gt;

&lt;p&gt;Each node in my configuration uses Centos 6.5 operating system and python and libselinux-python rpm has been installed.
The libselinux-python rpm package is required also to disable selinux configuration and it could be also installed by ansible.&lt;/p&gt;

&lt;p&gt;I install ansible on the first host of my hostgroup.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@cloudera01 ~]# yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;epel-release
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@cloudera01 ~]# yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;ansible&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I modify the ansible hosts file /etc/ansible/hosts as showed below. This file defines an hostgroup named “cloudera” and its hostnames list (these hostnames should be defined on /etc/hosts or in a DNS server )&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@cloudera01 ~]# &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/ansible/hosts
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera]
cloudera01
cloudera02
cloudera03
cloudera04&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;I create a playbook in /root/cloudera-prerequisites.yml as showed in the following text box.&lt;/p&gt;

&lt;p&gt;Playbooks are text file in YAML format containing a set of orchestration steps.&lt;/p&gt;

&lt;p&gt;This playbook defines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The group of servers involved in the orchestration&lt;/li&gt;
  &lt;li&gt;The remote username used for the ssh connection
-A list of orchestration step(task) that must be executed on each server of cloudera group. Each task define an ansible module and their parameter. You can found more detail about ansible modules on Ansible documentation &lt;a href=&quot;http://docs.ansible.com/modules.html&quot;&gt;http://docs.ansible.com/modules.html&lt;/a&gt; .&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@cloudera01 ~]# &lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;cloudera-prerequisites.yml
- hosts: cloudera
remote_user: root
tasks:
- selinux: &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;disabled
- sysctl: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;net.ipv6.conf.all.disable_ipv6 &lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;present
- sysctl: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;net.ipv6.conf.default.disable_ipv6 &lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;present
- sysctl: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;vm.swappiness &lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10 &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;present
- authorized_key: &lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root &lt;span class=&quot;nv&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{{ lookup('file', '/root/.ssh/id_rsa.pub') }}&quot;&lt;/span&gt;
- copy: &lt;span class=&quot;nv&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/hosts &lt;span class=&quot;nv&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/hosts &lt;span class=&quot;nv&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root &lt;span class=&quot;nv&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root &lt;span class=&quot;nv&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0644&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now I’m ready to execute my playbook applying cloudera prerequisites on each node of cloudera hostgroup.
I launch the command ansible-playbook using the playbook as arguments and using the flag “-k”.
The flags -k allow to insert the password of all cloudera hosts in interactive way.
Ansible playbook can be also executed in other environment where there is an password-less ssh connection or hosts with different root passwords.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@cloudera01 ~]# ansible-playbook cloudera-prerequisites.yml &lt;span class=&quot;nt&quot;&gt;-k&lt;/span&gt;
SSH password: &lt;span class=&quot;c&quot;&gt;## Insert root password of all cloudera hosts&lt;/span&gt;

PLAY &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera] &lt;span class=&quot;k&quot;&gt;***************************************************************&lt;/span&gt;

GATHERING FACTS &lt;span class=&quot;k&quot;&gt;***************************************************************&lt;/span&gt;
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera04]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera01]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera03]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera02]

TASK: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;selinux &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;disabled] &lt;span class=&quot;k&quot;&gt;************************************************&lt;/span&gt;
changed: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera04]
changed: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera03]
changed: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera01]
changed: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera02]

TASK: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;sysctl &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;net.ipv6.conf.all.disable_ipv6 &lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;present] &lt;span class=&quot;k&quot;&gt;******&lt;/span&gt;
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera01]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera04]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera03]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera02]

TASK: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;sysctl &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;net.ipv6.conf.default.disable_ipv6 &lt;span class=&quot;nv&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;present] &lt;span class=&quot;k&quot;&gt;***&lt;/span&gt;
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera04]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera03]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera01]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera02]

TASK: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;authorized_key &lt;span class=&quot;nv&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root &lt;span class=&quot;nv&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA8aFJa2vXcrt42PmzIT8/9rFc4JQHS7ElV7p11l7KrV3Kq9IqnPaWei+u6zJ0zTW/J1DvOalzzT23tMakAMPpsKm/LEAQnvKA3Ytc0K+vtHH7tJaAB0QJAoq2rBocj7R+RJtnU8VvQxRyCYELDYoTLLjCKBjvyDN7908ojuuqHdb4LpIiTnge5WcofpeD64P1J4PN6sYAu+nTC/ykg4a75iiuyoWuocwfRgS9i1aFdyHHnY40rB8/Er+vzn9bQRbNTYjwo8kEaQt1ZM4ZRjzhM3gUUwM0JUjeSDN3soA+Dq4tW052nxiL5xEWsCcTLcy5cd6fChzEQShPP8xnee8btw== root@cloudera01.example.com&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;***&lt;/span&gt;
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera01]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera03]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera02]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera04]

TASK: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;copy &lt;span class=&quot;nv&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/hosts &lt;span class=&quot;nv&quot;&gt;dest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/hosts &lt;span class=&quot;nv&quot;&gt;owner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root &lt;span class=&quot;nv&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root &lt;span class=&quot;nv&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0644] &lt;span class=&quot;k&quot;&gt;***&lt;/span&gt;
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera02]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera01]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera03]
ok: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;cloudera04]

PLAY RECAP &lt;span class=&quot;k&quot;&gt;********************************************************************&lt;/span&gt;
cloudera01 : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6 &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
cloudera02 : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6 &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
cloudera03 : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6 &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
cloudera04 : &lt;span class=&quot;nv&quot;&gt;ok&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6 &lt;span class=&quot;nv&quot;&gt;changed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;unreachable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;nv&quot;&gt;failed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Reboot each system in order to apply selinux configuration&lt;/p&gt;

&lt;p&gt;When a playbook is executed, Ansible generate a report containing detailed information about the execution of each tasks.
In this case ansible informs you that a reboot of each node is required in order to apply to reload selinux configuration.&lt;/p&gt;

&lt;p&gt;Ansible executes idempotent configuration, it means that the same playbook can be reapplied on an already configured node without modifying any configuration.
This feature can be useful to check configuration changes or when a new host is added in a hostgroup.&lt;/p&gt;</content><author><name></name></author><category term="automation" /><category term="cluster" /><category term="hadoop" /><category term="linux" /><summary type="html">Ansible is an opensource software for configuring and managing a server infrastructures. It allows multi-node software deployment, ad hoc task execution and configuration management. In this post I show how use ansible to deploy some Cloudera Hadoop 5 prerequisites to a large set of server.</summary></entry><entry><title type="html">Python unit testing introduction</title><link href="/2015/04/05/python-unit-testing-introduction.html" rel="alternate" type="text/html" title="Python unit testing introduction" /><published>2015-04-05T00:00:00+00:00</published><updated>2015-04-05T00:00:00+00:00</updated><id>/2015/04/05/python-unit-testing-introduction</id><content type="html" xml:base="/2015/04/05/python-unit-testing-introduction.html">&lt;p&gt;The python standard library includes the library &lt;a href=&quot;https://docs.python.org/2/library/unittest.html&quot;&gt;unitest&lt;/a&gt; from python’s version 2.1 .&lt;/p&gt;

&lt;p&gt;Unitest is a unit testing framework, it  allows to define unit tests for software written in python.
This library also called PyUnit belongs to &lt;a href=&quot;http://en.wikipedia.org/wiki/XUnit&quot;&gt;xUnit testing framework set&lt;/a&gt;, a group of testing framework based on SUnit test framework designed by Kent Beck in 1998 for Smalltalk language testing.&lt;/p&gt;

&lt;p&gt;This post contains the description of  some simple tests used to verify the correctness of sort function.&lt;/p&gt;

&lt;p&gt;The following text box contains the code that will be the object of the test in this post. It’s composed by a method called bubbleSort containing a simple implementation of &lt;a href=&quot;http://en.wikipedia.org/wiki/Bubble_sort&quot;&gt;bubble sort algorithm&lt;/a&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# bubblesort.py
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bubbleSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
               &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;aList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;On the following code box an example of test case for the bubble sort function is showed.&lt;/p&gt;

&lt;p&gt;Observe that BubbleSortTest inherits the class TestCase included in the module unittest.&lt;/p&gt;

&lt;p&gt;Furthermore two methods testRandomList and testEmptyList has been defined to execute two tests on the bubbleSort function. The former generates a list of random integers, sorts them using bubbleSort and python native sort functions and compares their results. The latter verifies that the result of sorting a empty list is a empty list too.&lt;/p&gt;

&lt;p&gt;The correctness of the output generated by bubblesort function is checked using two methods starting with the word assert. In this example only the method assertEqual is used, it verifies the equality between the two method’s argument. The full list of assert methods provided by unittest library is reported in the following &lt;a href=&quot;https://docs.python.org/2/library/unittest.html#unittest.TestCase.assertEqual&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# bubblesorttest.py&amp;lt;/pre&amp;gt;
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;unittest&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bubblesort&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bubbleSort&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BubbleSortTest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unittest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TestCase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;setUp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;testRandomList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_clone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bubbleSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_clone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assertEqual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_clone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;testEmptyList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bubbleSort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;assertEqual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tearDown&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;unittest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The methods setUp and tearDown are optional. They allow to initialize and to release resources used during the tests,respectively. If these two function are defined, their code is executed before and after the execution of each test method.&lt;/p&gt;

&lt;p&gt;Starting the test execution is a simple activity, the module bubblesorttest.py must be executed from python interpreter as showed in the following box.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;python bubblesort_test.py
..
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------------------&lt;/span&gt;
Ran 2 tests &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;0.000s

OK&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The official python documentation contains further details about unittest &lt;a href=&quot;https://docs.python.org/3/library/unittest.html&quot;&gt;library&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="python" /><summary type="html">The python standard library includes the library unitest from python’s version 2.1 .</summary></entry></feed>