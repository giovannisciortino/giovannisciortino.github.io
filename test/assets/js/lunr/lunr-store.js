var store = [{
        "title": "Clone git repo to a non-empty directory",
        "excerpt":"Cloning a git repo to a non-empty directory can be sometimes useful. For example, you can have a remote git repository containing already some files and you want merge them with files contained in a existing local directory. If you try to clone the remote repository to non-empty directory you...","categories": [],
        "tags": ["git"],
        "url": "/test/2015/04/03/clone_git_repo_to_a_non-empty_directory.html",
        "teaser": null
      },{
        "title": "Python lambda functions",
        "excerpt":"Python allows to define anonymous functions (i.e. function not bounded to a name). You can easily identify this functions’ type because are defined using the keyword lambda. The following example contains two functions f and g both returning the sum of the two arguments x and y. The function f...","categories": [],
        "tags": ["python"],
        "url": "/test/2015/04/03/python-lambda-functions.html",
        "teaser": null
      },{
        "title": "Python unit testing introduction",
        "excerpt":"The python standard library includes the library unitest from python’s version 2.1 . Unitest is a unit testing framework, it allows to define unit tests for software written in python. This library also called PyUnit belongs to xUnit testing framework set, a group of testing framework based on SUnit test...","categories": [],
        "tags": ["python"],
        "url": "/test/2015/04/05/python-unit-testing-introduction.html",
        "teaser": null
      },{
        "title": "HAProxy basic configuration on Ubuntu 14.04",
        "excerpt":"HAProxy is a free, very fast and reliable solution offering high availability, load balancing, and proxying for TCP and HTTP-based applications. It is particularly suited for very high traffic web sites. Over the years it has become the de-facto standard opensource software load balancer, is now shipped with most mainstream...","categories": [],
        "tags": ["haproxy","linux"],
        "url": "/test/2015/05/09/haproxy_basic_configuration_on_ubuntu_14.04.html",
        "teaser": null
      },{
        "title": "Install and configure GPFS 4.1 filesystem on Linux Centos 6.6",
        "excerpt":"The General Parallel File System (GPFS) is a high-performance clustered file system developed by IBM. It can be deployed in shared-disk infrastructure or in shared-nothing architecture. It’s used by many large company and in serveral supercomputers on the Top 500 List. GPFS allows to configure a high available filesystem allowing...","categories": [],
        "tags": ["cluster","linux"],
        "url": "/test/2015/05/09/install_and_configure_gpfs_4.1_filesystem_on_linux_centos_6.6.html",
        "teaser": null
      },{
        "title": "Install Cloudera prerequisites with Ansible",
        "excerpt":"Ansible is an opensource software for configuring and managing a server infrastructures. It allows multi-node software deployment, ad hoc task execution and configuration management. In this post I show how use ansible to deploy some Cloudera Hadoop 5 prerequisites to a large set of server. Ansible is a lightweight alternative...","categories": [],
        "tags": ["automation","cluster","hadoop","linux"],
        "url": "/test/2015/05/09/install_cloudera_prerequisites_with_ansible.html",
        "teaser": null
      },{
        "title": "HA two node GPFS cluster with tie-breaker disk",
        "excerpt":"In a previous post I described how configure a GPFS cluster filesystem ( a filesystem that can be mounted by two or more servers simultaneously ). This article describes the changes required to enable a high-availability configuration for a GPFS cluster filesystem. This configuration allows each node to write and...","categories": [],
        "tags": ["cluster","linux"],
        "url": "/test/2015/05/23/ha_two_node_gpfs_cluster_with_tie-breaker_disk.html",
        "teaser": null
      },{
        "title": "Flume: Import apache logs in hadoop hdfs",
        "excerpt":"Flume is a project of the Apache Software Foundation used to import stream of data to a centralized data store. In hadoop environments Flume is used to import data into hadoop clusters from different data sources. In this post I show how use Flume to import apache logs (access_log and...","categories": [],
        "tags": ["apache","flume","linux"],
        "url": "/test/2015/07/06/flume_import_apache_logs_in_hadoop_hdfs.html",
        "teaser": null
      },{
        "title": "Import Mysql data in Elasticsearch server",
        "excerpt":"Elasticsearch is a near real-time search server based on Lucene. It allows to create a distributed full-text search engine. It’s an opensource software developed in Java. It offers REST api in order to insert, retrieve and search data. In this post I describe how import data from a mysql database...","categories": [],
        "tags": ["cluster","linux"],
        "url": "/test/2016/01/06/import_mysql_data_in_elasticsearch_server.html",
        "teaser": null
      },{
        "title": "Apache Spark – Distributed computation of π in 8 lines of Python code",
        "excerpt":"In this post I show how to write a distributed application computing an approximation of pi number though a Spark application using Python. A brief introduction of Spark Apache Spark is an opensource cluster computing framework supporting developers to create distributed applications. Spark applications provides performance up to 100 times...","categories": [],
        "tags": ["hadoop","linux","python","spark"],
        "url": "/test/2016/02/16/apache_spark_distributed_computation_of_pi_in_8_lines_of_python_code.html",
        "teaser": null
      },{
        "title": "Distributed SQL Query using SparkSQL, HDFS and Sqoop",
        "excerpt":"Spark SQL: A brief introduction Spark SQL is a component of Spark framework. It allows to manipulate big unstructured data file and extract useful information using SQL. It introduces a new data abstraction called DataFrames allowing the analysis of structured and semi-structured data. Spark SQL provides API in Scala,Python and...","categories": [],
        "tags": ["cluster","hadoop","python","spark"],
        "url": "/test/2016/03/01/distrubuted_sql_query_using_sparksql_hdfs_sqoop.html",
        "teaser": null
      },{
        "title": "Use GitHub Actions to create Jekyll post from GitHub issue",
        "excerpt":"This post describe a GitHub Actions workflow that allow to create new post on a Jekyll web site contained in a GitHub repository using the issue editor of GitHub website. GitHub Actions [1] makes it easy to automate all your software workflows, it build, test, and deploy your code right...","categories": [],
        "tags": ["github","jekyll"],
        "url": "/test/2020/04/05/use_github_actions_to_create_jekyll_post_from_github_issue.html",
        "teaser": null
      },{
        "title": "Red Hat Satellite 6 reporting engine - System currency report",
        "excerpt":"This post describe how generate the “System currency” report in Red Hat Satellite 6. System currency report is a report existing in Satellite 5 but it isn’t included in Satellite 6. Below there is a description this report from Satellite 5 documentation System Currency report which lists registered systems ordered...","categories": [],
        "tags": ["satellite"],
        "url": "/test/2020/05/06/satellite_6_reporting_engine_system_currency_report.html",
        "teaser": null
      }]
